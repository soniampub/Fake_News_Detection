{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets understand more on TFIDF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A tf-idf vectorization of a corpus of text documents assigns each word in a document a number that is proportional to its frequency in the document and inversely proportional to the number of documents in which it occurs\n",
    "\n",
    "Very common words, such as “a” or “the”, thereby receive heavily discounted tf-idf scores, in contrast to words that are very specific to the document in question. The result is a matrix of tf-idf scores with one row per document and as many columns as there are different words in the dataset. (Ref https://buhrmann.github.io/tfidf-analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer , TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train_kaggle.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<class 'str'>, <class 'float'>], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'].apply(type).unique()\n",
    "# explore more why the text type is coming as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['text'].drop(df_train[df_train['text'].apply(type) == float].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<class 'str'>], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.apply(type).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_word_tokenize = TreebankWordTokenizer().tokenize\n",
    "tokens = [treebank_word_tokenize(content.lower()) for content in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords') # figure out a way to not download it every time?\n",
    "stop_word = set(stopwords.words('english')) #only unique stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change above X if we r using tokenize and other nlp process\n",
    "#Dropping the Nan values and info\n",
    "df_train.dropna(inplace=True)\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# do the similar think on headline (author ?? or source)\n",
    "# name these to test , validation\n",
    "X_train,  X_test,  y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer  = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,ngram_range=(1,2),max_df= 0.85, min_df= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes around 2-3 mins\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_validation_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "# train our model\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print most popular words/n-grams for each class/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_tfidf -> transformed X\n",
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_coef = lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40159966, -0.36543547,  0.0009544 , -0.03230355,  0.00609635,\n",
       "        0.04830219,  0.01201773,  0.02882158,  0.00061854,  0.02897177])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_coef[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see for each class what are the top n features or ngrams present. Lets start seeing a what features (in our case words or n-grams) are used to classify a document. For that we can write a function. This will user argsort to produce the indices that would sort the given row based on td idf, then reverse them (descending order) and then select top n features. We also need to format our row (which is sparse) to a format that argsort will accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_features_doc(tfidf_row_formated, feature_names, top_n=25):\n",
    "    tfidf_row = tfidf_row_formated\n",
    "    \n",
    "    #tfidf_row_formated = np.squeeze(tfidf_row.toarray()) # this is to format to np array that argsort will expect\n",
    "    top_n_index = np.argsort(tfidf_row)[::-1][:top_n] #It returns an array of indices of the same shape as a that index data along the given axis in sorted order.\n",
    "    top_feat_tuple = [(feature_names[i], tfidf_row[i]) for i in top_n_index]\n",
    "    return top_feat_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_no = 4\n",
    "tfidf_row_formated = np.squeeze(X_train_tfidf[doc_no].toarray())\n",
    "top_feat_tuple = top_n_features_doc(tfidf_row_formated, feature_names, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we can show what top words or features are used to classify this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smirnov', 0.27639183875467016),\n",
       " ('mr smirnov', 0.2570028427186608),\n",
       " ('russian', 0.20214192282192292),\n",
       " ('olympic', 0.18800806724843414),\n",
       " ('doping', 0.1833163107144178),\n",
       " ('athletes', 0.16785601016497628),\n",
       " ('antidoping', 0.14856183799588057),\n",
       " ('olympic committee', 0.13903913929811618),\n",
       " ('mclaren', 0.13756086910412713),\n",
       " ('mr', 0.13720973333629302)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feat_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14628, 743712)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.\n",
    "def top_features_all_docs(X_train_tfidf, row_ids = None, tfid_limit=0.1,  top_n=25):\n",
    "    # we can also fiter based on certain row ids \n",
    "    ''' returns top n features (words/n-grams) that on average are most common across all \n",
    "        all documents in our tfidf matrix if row_ids are None else it will filter based on row_ids\n",
    "    '''\n",
    "    if row_ids:\n",
    "        all_docs = X_train_tfidf[row_ids].toarray()\n",
    "    else:\n",
    "        print(\"Coming in this loop\")\n",
    "        all_docs = X_train_tfidf.toarray() # convert to np array\n",
    "    \n",
    "    all_docs[all_docs < tfid_limit ] = 0 \n",
    "    \n",
    "    all_docs_mean_tfidf_mean = np.mean(all_docs, axis=0)\n",
    "    \n",
    "    print(\"Coming here\", all_docs_mean_tfidf_mean.shape)\n",
    "    top_features = top_n_features_doc(all_docs_mean_tfidf_mean, feature_names)\n",
    "    return top_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coming in this loop\n",
      "Coming here (743712,)\n",
      "[('mr', 0.021452655758997248), ('trump', 0.015939575031755707), ('mr trump', 0.009288355530070494), ('clinton', 0.009029553712913552), ('ms', 0.004065133790661662), ('said', 0.004039320096436103), ('fbi', 0.0037453680090903424), ('hillary', 0.003669984100880582), ('russia', 0.0034117748323450763), ('police', 0.0033820732282593014), ('comey', 0.0032127156240300244), ('obama', 0.0030540825336704855), ('_____', 0.002968468247471249), ('mrs', 0.002627442958253107), ('mrs clinton', 0.002482043274480689), ('la', 0.002412407430076807), ('china', 0.002379252492807141), ('israel', 0.002241969853591411), ('russian', 0.002229520997511233), ('syria', 0.0020544469890717942), ('women', 0.0020179327362084337), ('emails', 0.001842932852766811), ('percent', 0.0018143032222292976), ('que', 0.0017961415038461447), ('el', 0.0017110091040987035)]\n"
     ]
    }
   ],
   "source": [
    "# 8.26\n",
    "print(top_features_all_docs(X_train_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now look into our all document set to see what are the top most important words across all the documents. For that we can take average of each word (feature/ngram) tfidf across all documents(rows). Also we can remove words with low tdidf, because commanly used word such as \"a\", \"the\", \"news\" (as this is news dataset) will have low tfidf values within each document but they occur in most of the documents so when we calculate their average, it will be a high average and will be picked up in pur top n calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also examine what are the top most features for each class. We can filter based on class label from our tfidf matrix and call our top_features_all_docs method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
