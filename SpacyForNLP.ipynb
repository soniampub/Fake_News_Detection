{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-the-box: download best-matching default model\n",
    "# python -m spacy download en\n",
    "parser = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        # get rid of newlines\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        \n",
    "        # replce mentions wth @\n",
    "        mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "        text = mentionFinder.sub(\"@MENTION\", text)\n",
    "\n",
    "        # replace emails and also @ mention\n",
    "        emailFinder = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\", re.IGNORECASE)\n",
    "        text = emailFinder.sub(\"<EMAIL>\", text)\n",
    "\n",
    "        # replace HTML symbols\n",
    "        text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "        \n",
    "        text = text.replace(\"(Before It's News)\", \"\")\n",
    "        text = text.replace(\"% of readers think this story is Fact. Add your two cents.\", \"\")\n",
    "        ## Exlpore more patterns inside text and add it here\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword that we dont want \n",
    "STOPLIST = set(stopwords.words('english')) # add more stopword\n",
    "# symbols that we dont want, research add more later \n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"’\", \"…\"]\n",
    "\n",
    "# Lets create a custome tokenizer using spacy\n",
    "def pretokenizer_clean(texts):\n",
    "    tokens = parser(texts)\n",
    "    lemmas = []\n",
    "    try: \n",
    "        lemmas = [tok.text.lower().strip() if tok.ent_type_ == \"\" else \"<{}>\".format(tok.ent_type_) for tok in tokens]\n",
    "    except:\n",
    "        print('error occured')\n",
    "        lemmas.append(\"<UNK>\")\n",
    "    \n",
    "    tokens  = lemmas\n",
    "    # For named entity we hve to replace them with their positional index\n",
    "    #tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    \n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "                \n",
    "    all_tokens = \" \".join(tokens)\n",
    "   \n",
    "    return str(all_tokens)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_pickle('data/all_data.pkl')\n",
    "#Do this once, only when your data is changed \n",
    "# df_train = df_all.sample(frac=0.7)\n",
    "# df_test = df_all.loc[~df_all.index.isin(df_train.index), :]\n",
    "# #pickle these for grid search\n",
    "# df_train.to_pickle('data/data_train.pkl')\n",
    "# df_test.to_pickle('data/data_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('data/data_train.pkl')\n",
    "df_test = pd.read_pickle('data/data_test.pkl')\n",
    "\n",
    "X_train = df_train['content']\n",
    "y_train = df_train['label']\n",
    "X_test = df_test['content']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6796,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [clean_text(text) for text in X_train]\n",
    "X_test = [clean_text(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = pretokenizer_clean(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#About 30 min \n",
    "X_train = [pretokenizer_clean(text) for text in X_train] \n",
    "X_test = [pretokenizer_clean(text) for text in X_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "tfidf_vectorizer  = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,  ngram_range=(1,2), max_df= 0.85, min_df= 2, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.972166464023758\n",
      "total time  9.972394943237305\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "import time\n",
    "start = timer()\n",
    "t0 = time.time()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "# STOP MY TIMER\n",
    "elapsed_time = timer() - start # in seconds\n",
    "print(elapsed_time)\n",
    "\n",
    "print(\"total time \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6796, 5000)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# using optimal parameters from grid search (l1 and 10)\n",
    "lr = LogisticRegression(penalty='l1', C = 10)\n",
    "# train our model\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict_lr = lr.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy and F1 score \n",
      "\n",
      "Accuracy 89.32\n",
      "F1 83.692\n",
      "Precision 84.444\n",
      "Recall 82.952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\n",
    "lr_acc = accuracy_score(y_test, y_test_predict_lr) *  100 \n",
    "lr_F1 = f1_score(y_test, y_test_predict_lr) * 100\n",
    "lr_precision = precision_score(y_test, y_test_predict_lr) * 100\n",
    "lr_recall = recall_score(y_test, y_test_predict_lr) * 100\n",
    "print (\"Logistic regression accuracy and F1 score \\n\")\n",
    "print (\"Accuracy {:.5}\".format(lr_acc))\n",
    "print (\"F1 {:.5}\".format(lr_F1))\n",
    "print (\"Precision {:.5}\".format(lr_precision))\n",
    "print (\"Recall {:.5}\".format(lr_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(X_train_tfidf, 'data/X_train_tfidf.pkl')\n",
    "save_object(X_test_tfidf, 'data/X_test_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(y_train, 'data/y_train_spacy.pkl')\n",
    "save_object(y_test, 'data/y_test_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(X_train, 'data/X_train_spacy.pkl')\n",
    "save_object(X_test, 'data/X_test_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "save_object(feature_names, 'data/tfidf_featurenames_spacy.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X_train_tfidf.pkl', 'rb') as input:\n",
    "    saved_xtrain = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['additional class',\n",
       " 'additional information',\n",
       " 'additional reporting',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adjusted',\n",
       " 'adjustment']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_index = np.argsort(coeff)[::-1][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "top_feat_tuple = [(feature_names[i], coeff[i]) for i in top_n_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 30.40274696359508),\n",
       " ('subscribe org', 28.91593141667666),\n",
       " ('post appeared', 24.075247732257935),\n",
       " ('article', 23.534869065678073),\n",
       " ('appeared ordinal', 22.713944886038636),\n",
       " ('originally published', 21.239162700965448),\n",
       " ('embedded content', 21.08176913887025),\n",
       " ('average cardinal', 18.44430605535015),\n",
       " ('wrong', 16.543722144762654),\n",
       " ('post', 16.031163248874066),\n",
       " ('bullish', 15.810049600853297),\n",
       " ('certainly', 15.255951826697286),\n",
       " ('vs', 15.070715248894407),\n",
       " ('copyright', 14.904340759721523),\n",
       " ('spiritual', 14.847342707158452),\n",
       " ('essays', 14.801456777513922),\n",
       " ('ultimately', 14.48958528963117),\n",
       " ('correct', 14.047396169186069),\n",
       " ('reading', 13.719835085792017),\n",
       " ('figure', 13.657053190569743),\n",
       " ('morning', 12.75193108030634),\n",
       " ('appears', 12.48842920615319),\n",
       " ('spirit', 12.466586242603228),\n",
       " ('finally', 12.422041837713445),\n",
       " ('exposed', 11.983662727441743),\n",
       " ('scenario', 11.658998959492983),\n",
       " ('proof', 11.571683735018052),\n",
       " ('middle', 11.468704576979368),\n",
       " ('terrorist', 11.392642527493187),\n",
       " ('mom', 11.17405632105193)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feat_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
