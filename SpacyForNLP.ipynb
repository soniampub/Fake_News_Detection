{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-the-box: download best-matching default model\n",
    "#python -m spacy download en\n",
    "parser = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train_kaggle.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      "id        20800 non-null int64\n",
      "title     20242 non-null object\n",
      "author    18843 non-null object\n",
      "text      20761 non-null object\n",
      "label     20800 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing float type:  (20800, 5)\n",
      "After removing float type:  (20761, 5)\n"
     ]
    }
   ],
   "source": [
    "df_train['text'].apply(type).unique()\n",
    "# explore more why the text type is coming as float\n",
    "\n",
    "print(\"Before removing float type: \",df_train.shape)\n",
    "# only remove rows with text as float type\n",
    "\n",
    "df_train = df_train.drop(df_train[df_train['text'].apply(type) == float].index)\n",
    "print(\"After removing float type: \", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove new lines \n",
    "    text = text.strip().replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    \n",
    "    # remove html tags\n",
    "    text = text.replace(\"&amp;\", \"and\")\n",
    "    text = text.replace(\"&gt;\", \">\")\n",
    "    text = text.replace(\"&lt;\", \"<\")\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text before passing it to spacy tokenizer\n",
    "cleaned_text = [ clean_text(text) for text in df_train[\"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-29-8aaff6842c1a>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-8aaff6842c1a>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    tokens = parser(texts)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# stopword that we dont want \n",
    "STOPLIST = set(stopwords.words('english')) # add more stopword\n",
    "# symbols that we dont want, research add more later \n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\"]\n",
    "\n",
    "# Lets create a custome tokenizer using spacy\n",
    "def spacy_tokenizer(texts):\n",
    "    #tokens = parser(sentence)\n",
    "    # lets replace named entity, if a token is a place replace with <PLACE> if a name with<NAME>\n",
    "    \n",
    "    # we need dict to store mapping of each document and their tokens, we also need a hastable or lookup\n",
    "    # to keep track of their ids\n",
    "    token_dict = {}\n",
    "    hash_table = {}\n",
    "    len_texts = len(texts)\n",
    "    print(len_texts)\n",
    "    \n",
    "    \n",
    "#     for i, text in enumerate(texts):\n",
    "#         hash_table[text] = i\n",
    "    \n",
    "#     print(hash_table)         \n",
    "#     for document in parser.pipe(texts, batch_size=1000, n_threads=4):\n",
    "#         tok_list = []\n",
    "#         for doc in document:\n",
    "#             try:\n",
    "#                 tok_list.append(doc.text().lower().strip() if doc.ent_type_ == \"\" else \"<{}>\".format(doc.ent_type_))\n",
    "#             except:\n",
    "#                 #print(\"Erro when identifying NER, setting to UNKOWN type\")\n",
    "#                 tok_list.append(\"<UNKOWN>\")\n",
    "#                 continue\n",
    "        \n",
    "#         tokens =  tok_list \n",
    "\n",
    "        tokens = parser(texts)\n",
    "        tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "        tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "        tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "        \n",
    "        while \"\" in tokens:\n",
    "            tokens.remove(\"\")\n",
    "        while \" \" in tokens:\n",
    "            tokens.remove(\" \")\n",
    "        while \"\\n\" in tokens:\n",
    "            tokens.remove(\"\\n\")\n",
    "        while \"\\n\\n\" in tokens:\n",
    "            tokens.remove(\"\\n\\n\")\n",
    "                \n",
    "        all_tokens = \" \".join(tokens)\n",
    "        token_dict[document.text] = str(all_tokens)\n",
    "    \n",
    "    # do a reverse lookup\n",
    "    return_tokens = list(range(len_texts))\n",
    "    for text, i in hash_table.items():\n",
    "        try:\n",
    "            return_tokens[i] = token_dict[text]\n",
    "        except Exception as e:\n",
    "            print(\"Tokenize Text error, setting to None\")\n",
    "            return_tokens[i] = \"None\"\n",
    "            continue\n",
    "    \n",
    "    #print(return_tokens[:2])\n",
    "    return return_tokens\n",
    "\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword that we dont want \n",
    "STOPLIST = set(stopwords.words('english')) # add more stopword\n",
    "# symbols that we dont want, research add more later \n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\"]\n",
    "\n",
    "# Lets create a custome tokenizer using spacy\n",
    "def spacy_simple_tokenizer(texts):\n",
    "    #tokens = parser(sentence)\n",
    "\n",
    "    tokens = parser(texts)\n",
    "    \n",
    "#   print(dir(tokens))\n",
    "#   print(set([w for w in tokens.ents]))\n",
    "#   tokens = [tok.text.lower().strip() if tok.ent_type_ == \"\" else \"<{}>\".format(tok.ent_type_) for tok in tokens]\n",
    "     \n",
    "#     for ent in tokens.ents:\n",
    "#         print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        \n",
    "#     for tok in tokens:\n",
    "#         if tok.ent_type_ != \"\":\n",
    "#             print(tok.ent_type_)\n",
    "        \n",
    "    # For named entity we hve to replace them with their positional index\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    \n",
    "       \n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "                \n",
    "    all_tokens = \" \".join(tokens)\n",
    "   \n",
    "    return str(all_tokens)\n",
    "\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#change above X if we r using tokenize and other nlp process\n",
    "#Dropping the Nan values and info\n",
    "df_train.dropna(inplace=True)\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# do the similar think on headline (author ?? or source)\n",
    "# name these to test , validation\n",
    "X_train,  X_test,  y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one_df = df_train['text'].iloc[0]\n",
    "#only_one_df = \"this is a cat, and a cat walked over to mountain and said what a lovely day is, and she was in China.\\n Is India a nice place. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#started at 11.54 -> took almost 1 hour \n",
    "for docs in df_train['text']:\n",
    "    return_token = spacy_simple_tokenizer(docs)\n",
    "    #return_token \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer  = TfidfVectorizer(tokenizer= spacy_simple_tokenizer, ngram_range=(1,1), max_df= 0.85, min_df= 2, max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# started at 12.46\n",
    "from timeit import default_timer as timer\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_validation_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "# STOP MY TIMER\n",
    "elapsed_time = timer() - start # in seconds\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
