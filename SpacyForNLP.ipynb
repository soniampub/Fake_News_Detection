{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-the-box: download best-matching default model\n",
    "# python -m spacy download en\n",
    "parser = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        # get rid of newlines\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        \n",
    "        # replce mentions wth @\n",
    "        mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "        text = mentionFinder.sub(\"@MENTION\", text)\n",
    "\n",
    "        # replace emails and also @ mention\n",
    "        emailFinder = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\", re.IGNORECASE)\n",
    "        text = emailFinder.sub(\"<EMAIL>\", text)\n",
    "\n",
    "        # replace HTML symbols\n",
    "        text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "        \n",
    "        ## Exlpore more patterns inside text and add it here\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword that we dont want \n",
    "STOPLIST = set(stopwords.words('english')) # add more stopword\n",
    "# symbols that we dont want, research add more later \n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"’\", \"’s\", \"'s\"]\n",
    "\n",
    "# Lets create a custome tokenizer using spacy\n",
    "def spacy_simple_tokenizer(texts):\n",
    "    tokens = parser(texts)\n",
    "    lemmas = []\n",
    "    try: \n",
    "        lemmas = [tok.text.lower().strip() if tok.ent_type_ == \"\" else \"<{}>\".format(tok.ent_type_) for tok in tokens]\n",
    "    except:\n",
    "        print('error occured')\n",
    "        lemmas.append(\"<UNK>\")\n",
    "    \n",
    "    tokens  = lemmas\n",
    "    # For named entity we hve to replace them with their positional index\n",
    "    #tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    \n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "                \n",
    "    all_tokens = \" \".join(tokens)\n",
    "   \n",
    "    return str(all_tokens)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('data/data_train.pkl')\n",
    "df_test = pd.read_pickle('data/data_test.pkl')\n",
    "\n",
    "X_train = df_train['content']\n",
    "y_train = df_train['label']\n",
    "X_test = df_test['content']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8744,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [clean_text(text) for text in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4075     Cbrain A/S : \\n\\n* The Ministry of Immigration...\n",
       "100      (Before It's News)  \\n\\nFor the last several m...\n",
       "8141     Story highlights Injured firefighter chats wit...\n",
       "880      Z Capital Group said its private equity manage...\n",
       "11936    (Before It's News)  \\n \\nWhen Youngest Junior ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one_df = X_train[4075]\n",
    "#only_one_df = \"this is a cat, and a cat walked over to mountain and said what a lovely day is, and she was in China.\\n Is India a nice place. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = spacy_simple_tokenizer(only_one_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cbrain <ORG> <ORG> <ORG> <ORG> <ORG> <ORG> <ORG> <ORG> signed <GPE> agreement <GPE> deliver <PRODUCT> case document management <ORG> delivered cloud service saas <ORG> <FAC> <FAC> <GPE> <NORP> agency <ORG> services <PERSON> source text eikon <ORG> company coverage <PERSON> <PERSON>'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer  = TfidfVectorizer(tokenizer= spacy_simple_tokenizer, ngram_range=(1,2), max_df= 0.85, min_df= 2, max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-c81b580437bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# STOP MY TIMER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;31m# in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "# started at 8.46\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "start = timer()\n",
    "t0 = time.time()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_validation_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "# STOP MY TIMER\n",
    "elapsed_time = timer() - start # in seconds\n",
    "print(elapsed_time)\n",
    "\n",
    "print(\"total time \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8744, 2883)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# using optimal parameters from grid search (l1 and 10)\n",
    "lr = LogisticRegression(penalty='l1', C = 10)\n",
    "# train our model\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict_lr = lr.predict(X_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy and F1 score \n",
      "\n",
      "Accuracy 93.436\n",
      "F1 86.195\n",
      "Precision 88.377\n",
      "Recall 84.118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\n",
    "lr_acc = accuracy_score(y_test, y_test_predict_lr) *  100 \n",
    "lr_F1 = f1_score(y_test, y_test_predict_lr) * 100\n",
    "lr_precision = precision_score(y_test, y_test_predict_lr) * 100\n",
    "lr_recall = recall_score(y_test, y_test_predict_lr) * 100\n",
    "print (\"Logistic regression accuracy and F1 score \\n\")\n",
    "print (\"Accuracy {:.5}\".format(lr_acc))\n",
    "print (\"F1 {:.5}\".format(lr_F1))\n",
    "print (\"Precision {:.5}\".format(lr_precision))\n",
    "print (\"Recall {:.5}\".format(lr_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(X_train_tfidf, 'data/X_train_tfidf.pkl')\n",
    "save_object(X_validation_tfidf, 'data/X_test_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(y_train, 'data/y_train_spacy.pkl')\n",
    "save_object(y_test, 'data/y_test_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X_train_tfidf.pkl', 'rb') as input:\n",
    "    saved_xtrain = pickle.load(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
