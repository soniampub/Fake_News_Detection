{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-the-box: download best-matching default model\n",
    "# python -m spacy download en\n",
    "parser = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        # get rid of newlines\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        \n",
    "        # replce mentions wth @\n",
    "        mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "        text = mentionFinder.sub(\"@MENTION\", text)\n",
    "\n",
    "        # replace emails and also @ mention\n",
    "        emailFinder = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\", re.IGNORECASE)\n",
    "        text = emailFinder.sub(\"<EMAIL>\", text)\n",
    "\n",
    "        # replace HTML symbols\n",
    "        text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "        \n",
    "        # these lines were occuring in most of the fake news articles \n",
    "        text = text.replace(\"(Before It's News)\", \"\")\n",
    "        text = text.replace(\"% of readers think this story is Fact. Add your two cents.\", \"\")\n",
    "        ## Exlpore more patterns inside text and add it here\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword that we dont want \n",
    "STOPLIST = set(stopwords.words('english')) # add more stopword\n",
    "# symbols that we dont want, research add more later \n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"’\", \"…\"]\n",
    "\n",
    "# Lets create a custome tokenizer using spacy\n",
    "def pretokenizer_clean(texts):\n",
    "    tokens = parser(texts)\n",
    "    lemmas = []\n",
    "    try: \n",
    "        lemmas = [tok.text.lower().strip() if tok.ent_type_ == \"\" else \"<{}>\".format(tok.ent_type_) for tok in tokens]\n",
    "    except:\n",
    "        print('error occured')\n",
    "        lemmas.append(\"<UNK>\")\n",
    "    \n",
    "    tokens  = lemmas\n",
    "    # For named entity we hve to replace them with their positional index\n",
    "    #tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    \n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "                \n",
    "    all_tokens = \" \".join(tokens)\n",
    "   \n",
    "    return str(all_tokens)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_pickle('data/all_data.pkl')\n",
    "#Do this once, only when your data is changed \n",
    "# df_train = df_all.sample(frac=0.7)\n",
    "# df_test = df_all.loc[~df_all.index.isin(df_train.index), :]\n",
    "# #pickle these for grid search\n",
    "# df_train.to_pickle('data/data_train.pkl')\n",
    "# df_test.to_pickle('data/data_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('data/data_train.pkl')\n",
    "df_test = pd.read_pickle('data/data_test.pkl')\n",
    "\n",
    "X_train = df_train['content']\n",
    "y_train = df_train['label']\n",
    "X_test = df_test['content']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9708, 7)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6796,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [clean_text(text) for text in X_train]\n",
    "X_test = [clean_text(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = pretokenizer_clean(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#About 30 min \n",
    "X_train = [pretokenizer_clean(text) for text in X_train] \n",
    "X_test = [pretokenizer_clean(text) for text in X_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "tfidf_vectorizer  = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,  ngram_range=(1,2), max_df= 0.85, min_df= 2, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.972166464023758\n",
      "total time  9.972394943237305\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "import time\n",
    "start = timer()\n",
    "t0 = time.time()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "# STOP MY TIMER\n",
    "elapsed_time = timer() - start # in seconds\n",
    "print(elapsed_time)\n",
    "\n",
    "print(\"total time \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6796, 5000)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(X_train_tfidf, 'data/X_train_tfidf.pkl')\n",
    "save_object(X_test_tfidf, 'data/X_test_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(y_train, 'data/y_train_spacy.pkl')\n",
    "save_object(y_test, 'data/y_test_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(X_train, 'data/X_train_spacy.pkl')\n",
    "save_object(X_test, 'data/X_test_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(tfidf_vectorizer.get_feature_names(), 'data/tfidf_featurename.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
